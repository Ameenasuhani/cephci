tests:
   - test:
      name: install ceph pre-requisites
      module: install_prereq.py
      abort-on-fail: true

   - test:
      name: ceph ansible
      polarion-id: CEPH-83571467
      module: test_ansible.py
      config:
        is_mixed_lvm_configs: True
        ansi_config:
            ceph_origin: distro
            ceph_stable_release: nautilus
            ceph_repository: rhcs
            osd_scenario: lvm
            fetch_directory: ~/fetch
            copy_admin_key: true
            dashboard_enabled: True
            dashboard_admin_user: admin
            dashboard_admin_password: p@ssw0rd
            grafana_admin_user: admin
            grafana_admin_password: p@ssw0rd
            node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.1
            grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
            prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:4.1
            alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:4.1 
            ceph_conf_overrides:
                global:
                  osd_pool_default_pg_num: 64
                  osd_default_pool_size: 2
                  osd_pool_default_pgp_num: 64
                  mon_max_pg_per_osd: 1024
                mon:
                  mon_allow_pool_delete: true
                client:
                  rgw crypt require ssl: false
                  rgw crypt s3 kms encryption keys: testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                    testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            cephfs_pools:
              - name: "cephfs_data"
                pgs: "8"
              - name: "cephfs_metadata"
                pgs: "8"
      desc: osd with 6 osd scenarios with lvm
      destroy-cluster: False
      abort-on-fail: true

   - test:
      name: config roll over rgw daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          is_mixed_lvm_configs: True
          ansi_config:
              ceph_origin: distro
              ceph_stable_release: nautilus
              ceph_repository: rhcs
              osd_scenario: lvm
              osd_auto_discovery: False
              fetch_directory: ~/fetch
              copy_admin_key: true
              dashboard_enabled: True
              dashboard_admin_user: admin
              dashboard_admin_password: p@ssw0rd
              grafana_admin_user: admin
              grafana_admin_password: p@ssw0rd
              node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.1
              grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
              prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:4.1
              alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:4.1
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node5.*
                  demon:
                      - rgw
      desc: add rgw daemon

   - test:
      name: config roll over mds daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          is_mixed_lvm_configs: True
          ansi_config:
              ceph_origin: distro
              ceph_stable_release: nautilus
              ceph_repository: rhcs
              osd_scenario: lvm
              osd_auto_discovery: False
              fetch_directory: ~/fetch
              copy_admin_key: true
              dashboard_enabled: True
              dashboard_admin_user: admin
              dashboard_admin_password: p@ssw0rd
              grafana_admin_user: admin
              grafana_admin_password: p@ssw0rd
              node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.1
              grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
              prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:4.1
              alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:4.1
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node6.*
                  demon:
                      - mds
      desc: add mds daemon

   - test:
      name: config roll over client daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          is_mixed_lvm_configs: True
          ansi_config:
              ceph_origin: distro
              ceph_stable_release: nautilus
              ceph_repository: rhcs
              osd_scenario: lvm
              osd_auto_discovery: False
              fetch_directory: ~/fetch
              copy_admin_key: true
              dashboard_enabled: True
              dashboard_admin_user: admin
              dashboard_admin_password: p@ssw0rd
              grafana_admin_user: admin
              grafana_admin_password: p@ssw0rd
              node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.1
              grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
              prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:4.1
              alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:4.1
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node7.*
                  demon:
                      - client
      desc: add client

   - test:
      name: config roll over nfs daemon
      polarion-id: CEPH-9581
      module: test_ansible_roll_over.py
      config:
          is_mixed_lvm_configs: True
          ansi_config:
              ceph_origin: distro
              ceph_stable_release: nautilus
              ceph_repository: rhcs
              osd_scenario: lvm
              osd_auto_discovery: False
              fetch_directory: ~/fetch
              copy_admin_key: true
              dashboard_enabled: True
              dashboard_admin_user: admin
              dashboard_admin_password: p@ssw0rd
              grafana_admin_user: admin
              grafana_admin_password: p@ssw0rd
              node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.1
              grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
              prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:4.1
              alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:4.1
              ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
              cephfs_pools:
                - name: "cephfs_data"
                  pgs: "8"
                - name: "cephfs_metadata"
                  pgs: "8"
          add:
              - node:
                  node-name: .*node8.*
                  demon:
                      - nfs
      desc: add nfs
