tests:
- test:
    name: install ceph pre-requisites
    module: install_prereq.py
    abort-on-fail: True

- test:
    name: ceph ansible install rhcs 3.x cdn
    module: test_ansible.py
    polarion-id: CEPH-83574033
    config:
      use_cdn: True
      build: '3.x'
      ansi_config:
        ceph_origin: repository
        ceph_repository: rhcs
        ceph_repository_type: cdn
        ceph_rhcs_version: 3
        osd_scenario: collocated
        osd_auto_discovery: False
        copy_admin_key: True
        containerized_deployment: true
        ceph_docker_image: "rhceph/rhceph-3-rhel7"
        ceph_docker_image_tag: "latest"
        ceph_docker_registry: "registry.redhat.io"
        ceph_conf_overrides:
          global:
            osd_pool_default_pg_num: 128
            osd_default_pool_size: 2
            osd_pool_default_pgp_num: 128
            mon_max_pg_per_osd: 4096
            mon_allow_pool_delete: True
          client:
            rgw crypt require ssl: false
            rgw crypt s3 kms encryption keys:
              testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
              testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
    desc: test cluster ceph-disk 3.x cdn setup using ceph-ansible
    abort-on-fail: True

- test:
      name: Parallel run
      module: test_parallel.py
      parallel:
#        - test:
#            name: librbd workunit
#            module: test_workunit.py
#            config:
#              test_name: rbd/test_librbd_python.sh
#              branch: nautilus
#              role: mon
#            desc: Test librbd unit tests
#        - test:
#            name: rados_bench_test
#            module: radosbench.py
#            config:
#              pg_num: '128'
#              pool_type: 'normal'
#            desc: run rados bench for 360 - normal profile
        - test:
            name: Upgrade containerized ceph to 4.x latest
            polarion-id: CEPH-83574033
            module: test_ansible_upgrade.py
            config:
              ansi_config:
                ceph_origin: distro
                ceph_stable_release: nautilus
                ceph_repository: rhcs
                ceph_rhcs_version: 4
                osd_scenario: lvm
                osd_auto_discovery: False
                ceph_stable: True
                ceph_stable_rh_storage: True
                fetch_directory: ~/fetch
                copy_admin_key: true
                containerized_deployment: true
                upgrade_ceph_packages: True
                dashboard_enabled: True
                dashboard_admin_user: admin
                dashboard_admin_password: p@ssw0rd
                grafana_admin_user: admin
                grafana_admin_password: p@ssw0rd
                node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:v4.6
                grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:4
                prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:v4.6
                alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:v4.6
                ceph_conf_overrides:
                  global:
                    osd_pool_default_pg_num: 64
                    osd_default_pool_size: 2
                    osd_pool_default_pgp_num: 64
                    mon_max_pg_per_osd: 1024
                  mon:
                    mon_allow_pool_delete: true
                  client:
                    rgw crypt require ssl: false
                    rgw crypt s3 kms encryption keys:
                      testkey-1=YmluCmJvb3N0CmJvb3N0LWJ1aWxkCmNlcGguY29uZgo=
                      testkey-2=aWIKTWFrZWZpbGUKbWFuCm91dApzcmMKVGVzdGluZwo=
            desc: Ceph-Ansible container upgrade 3.x cdn to 4.x latest
            abort-on-fail: True

#- test:
#    name: librbd workunit
#    module: test_workunit.py
#    config:
#      test_name: rbd/test_librbd_python.sh
#      branch: nautilus
#      role: mon
#    desc: Test librbd unit tests

#- test:
#    name: check-ceph-health
#    module: exec.py
#    config:
#      cmd: ceph -s
#      sudo: True
#    desc: Check for ceph health debug info
#
#- test:
#    name: rados_bench_test
#    module: radosbench.py
#    config:
#      pg_num: '128'
#      pool_type: 'normal'
#    desc: run rados bench for 360 - normal profile

- test:
    name: config roll over mon daemon
    polarion-id: CEPH-958
    module: test_ansible_roll_over.py
    config:
        add:
          - node:
              node-name: .*node11.*
              daemon:
                 - mon
    desc: add mon

#- test:
#    name: config roll over new osd daemon
#    polarion-id: CEPH-9583
#    module: test_ansible_roll_over.py
#    config:
#        add:
#          - node:
#              node-name: .*node3.*
#              daemon:
#                 - osd
#    desc: add new containerized osd node
#
#- test:
#    name: config roll over client daemon
#    polarion-id: CEPH-83573550
#    module: test_ansible_roll_over.py
#    config:
#        add:
#          - node:
#              node-name: .*node6.*
#              daemon:
#                  - client
#    desc: add client daemon
#
#- test:
#    name: config roll over rgw daemon
#    polarion-id: CEPH-9581
#    module: test_ansible_roll_over.py
#    config:
#        add:
#          - node:
#              node-name: .*node5.*
#              daemon:
#                  - rgw
#    desc: add rgw
#
#- test:
#    name: config roll over mds daemon
#    polarion-id: CEPH-9581
#    module: test_ansible_roll_over.py
#    config:
#        add:
#          - node:
#              node-name: .*node6.*
#              daemon:
#                  - mds
#    desc: add mds
#
#- test:
#    name: config roll over nfs daemon
#    polarion-id: CEPH-9581
#    module: test_ansible_roll_over.py
#    config:
#        add:
#          - node:
#              node-name: .*node8.*
#              daemon:
#                  - nfs
#    desc: add nfs
#
#- test:
#    name: shrink mon
#    polarion-id: CEPH-9584
#    module: shrink_mon.py
#    config:
#        mon-to-kill:
#            - .*node4.*
#    desc: remove monitor
#
#- test:
#    name: ceph ansible purge
#    module: purge_cluster.py
#    config:
#        ansible-dir: /usr/share/ceph-ansible
#        playbook-command: purge-docker-cluster.yml -e ireallymeanit=yes -e remove_packages=yes
#    desc: Purge ceph cluster
#
#- test:
#    name: Check for old container directories
#    module: bug_1834974.py
#    desc: Check for old container directories
#    destroy-cluster: True
